{"meta":{"title":"yinqm_Blog","subtitle":"书不成字，纸短情长","description":"小破站还没设计好，先将就着，哈哈哈哈","author":"Eric Yin","url":"http://yinqm.com","root":"/"},"pages":[{"title":"","date":"2020-03-21T05:54:48.989Z","updated":"2020-03-21T05:54:48.989Z","comments":false,"path":"categories/index.html","permalink":"http://yinqm.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-03-21T05:54:31.623Z","updated":"2020-03-21T05:54:31.623Z","comments":false,"path":"tags/index.html","permalink":"http://yinqm.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MachineLearning04","slug":"MachineLearning03","date":"2020-03-22T03:11:37.000Z","updated":"2020-03-22T03:43:01.387Z","comments":true,"path":"2020/03/22/MachineLearning03/","link":"","permalink":"http://yinqm.com/2020/03/22/MachineLearning03/","excerpt":"","text":"线性回归P1最小二乘法 &amp; P2概率视角 最小二乘法其实又叫最小平方法，是一种数据拟合的优化技术。实质上是利用最小误差的平方寻求数据的最佳匹配函数，利用最小二乘法可以便捷的求得未知的数据，起到预测的作用，并且是的这些预测的数据与实际数据之间的误差平方和达到最小。一般应用在曲线拟合的目的上。 拟合示意图python代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#%%import matplotlib import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_modelimport pandas as pd # Load CSV and columnsdf = pd.read_csv(\"Housing.csv\") Y = df['price']X = df['lotsize'] X=X.values.reshape(len(X),1)Y=Y.values.reshape(len(Y),1) # Split the data into training/testing setsX_train = X[:-250]X_test = X[-250:] # Split the targets into training/testing setsY_train = Y[:-250]Y_test = Y[-250:] # Create linear regression objectregr = linear_model.LinearRegression() # Train the model using the training setsregr.fit(X_train, Y_train) # Plot outputsplt.scatter(X_test, Y_test, color='black')plt.title('Test Data')plt.xlabel('Size')plt.ylabel('Price')plt.xticks(())plt.yticks(()) # Plot outputsplt.plot(X_test, regr.predict(X_test), color='red',linewidth=3) plt.show()#%%# 画出训练集的拟合曲线，第一种方法predict# Plot outputsplt.scatter(X_train, Y_train, color='black')plt.title('Train Data')plt.xlabel('Size')plt.ylabel('Price')plt.xticks(())plt.yticks(()) # Plot outputsplt.plot(X_train, regr.predict(X_train), color='red',linewidth=3) plt.show() #%%# 画出训练集的拟合曲线，第二种方法coef_*x+intercept_ # Plot outputsplt.scatter(X_train, Y_train, color='black')plt.title('Test Data')plt.xlabel('Size')plt.ylabel('Price')plt.xticks(())plt.yticks(()) # Plot outputsplt.plot(X_train, regr.coef_*X_train+regr.intercept_, color='red',linewidth=3) plt.show() #%%# 查看coef_*x+intercept_的值和regr.predict()的值是否相等，相减为0regr.coef_*X_train+regr.intercept_ - regr.predict(X_train) P3正则化 &amp; 正则化几何解释 正则化看起来有些抽象，其直译”规则化”，本质其实很简单，就是给模型加一些规则限制，约束要优化参数，目的是防止过拟合。其中最常见的规则限制就是添加先验约束，其中L1相当于添加Laplace先验，L相当于添加Gaussian先验。 L1正则是在原始的loss函数上加上一个L1正则化项，这个L1正则项实际就是在loss函数上添加一个结构化风险项，因此正则化其实和“带约束的目标函数”是等价的。而L1正则项就是一个1范数，本质相当于添加一个Laplace先验知识。同理，L2正则化项是一个2范数，本质却相当于添加一个Gaussian先验知识。 过拟合的时候，拟合函数的系数往往非常大，为什么？如P4所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。 一般来说，监督学习可以看做最小化P3的损失函数，第一项L衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。其中这个规则化函数就是我们常见的L0,L1,L2范数。 正则化的目的：防止过拟合！ 正则化的本质：约束（限制）要优化的参数。 贝叶斯决策理论是主观贝叶斯派归纳理论的重要组成部分。 贝叶斯决策就是在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策。贝叶斯决策理论方法是统计模型决策中的一个基本方法，其基本思想是： 已知类条件概率密度参数表达式和先验概率 利用贝叶斯公式转换成后验概率 根据后验概率大小进行决策分类 从贝叶斯角度理解正则化从我们平时最为熟悉的最小二乘回归、Ridge回归和LASSO回归入手。从概率论的角度：Least Square的解析解可以用Gaussian分布以及最大似然估计求得Ridge回归可以用Gaussian分布和最大后验估计解释LASSO回归可以用Laplace分布和最大后验估计解释给定观察数据,贝叶斯方法通过最大化后验概率估计参数w。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yinqm.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学理论","slug":"数学理论","permalink":"http://yinqm.com/tags/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"}]},{"title":"MachineLearning03","slug":"MachineLearning04","date":"2020-03-21T15:28:25.000Z","updated":"2020-03-21T16:40:32.728Z","comments":true,"path":"2020/03/21/MachineLearning04/","link":"","permalink":"http://yinqm.com/2020/03/21/MachineLearning04/","excerpt":"","text":"线性分类P1线性分类概述 &amp; P2感知机 分类是机器学习中的一个核心问题，在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。从数据中学习一个分类模型或分类决策函数，称为分类器。分类器对新的输入进行输出的预测，称为分类，可能的输出称为类，分类的类别为多个时，称为多类分类问题，主要研究的都为二分类问题。许多机器学习方法都是可以用来解分类问题，比如：感知机、朴素贝叶斯、支持向量机、决策树、逻辑回归、AdaBoost、神经网络等等。感知机是神经网络和支持向量机的基础，所以把感知机弄清楚对于其它算法的理解还是挺有必要的。 感知机是一种广泛使用的线性分类器。就是说数据必须是线性可分的(虽然实际中很难达到要求，但是可以通过一些方法将数据集变为线性可分)，属于判别模型(Discriminative Model)。感知器可谓是最简单的人工神经网络，只有一个神经元。感知器是对生物神经元的简单数学模拟，有与生物神经元相对应的部件，如权重（突触）、偏置（阈值）及激活函数（细胞体），输出为+1或-1。感知器是一种简单的两类线性分类模型 线性分类 P3线性判别分析 线性判别分析(linear discriminant analysis，LDA)是对费舍尔的线性鉴别方法的归纳，这种方法使用统计学，模式识别和机器学习方法，试图找到两类物体或事件的特征的一个线性组合，以能够特征化或区分它们。所得的组合可用来作为一个线性分类器，或者，更常见的是，为后续的分类做降维处理。 假设我们对一张100*100像素的图片做人脸识别，每个像素是一个特征，那么会有10000个特征，而对应的类别标签y仅仅是0/1值，1代表是人脸。这么多特征不仅训练复杂，而且不必要特征对结果会带来不可预知的影响，但我们想得到降维后的一些最佳特征（与y关系最密切的）。 主要思想是将一个高维空间中的数据投影到一个较低维的空间中，且投影后要保证各个类别的类内方差小而类间均值差别大，这意味着同一类的高维数据投影到低维空间后相同类别的聚在一起，而不同类别之间相距较远。 线性分类 P4目标函数分析 &amp; P5逻辑回归 线性判别分析（LDA）是一种有监督的数据降维算法，它的目标是最大化类间差异，最小化类内差异。数据经过投影之后，在低维空间里，同类样本聚集在一起，不同类的样本相距尽可能远。类内差异用每个类的方差来衡量，类间差异用各个类的类中心之间的距离来衡量，二者的比值为我们要优化的目标。由此构造出如P4中的损失函数，求解这一问题最后归结为求解矩阵的特征值问题。前面介绍的感知器算法的目标函数，欧氏距离损失，代表的都是经验风险，即在训练样本集上让误差最小化。这样做的泛化性能不一定好，还有一种做法为结构化风险最小化，典型代表是支持向量机。 logistic回归又称logistic回归分析，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。例如，探讨引发疾病的危险因素，并根据危险因素预测疾病发生的概率等。以胃癌病情分析为例，选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群必定具有不同的体征与生活方式等。因此因变量就为是否胃癌，值为“是”或“否”，自变量就可以包括很多了，如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。然后通过logistic回归分析，可以得到自变量的权重，从而可以大致了解到底哪些因素是胃癌的危险因素。同时根据该权值可以根据危险因素预测一个人患癌症的可能性。是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。 线性分类 P6高斯判别分析 &amp; P7最大似然函数 高斯判别分析的作用也是用于分类。对于两类样本，其服从伯努利分布，而对每个类中的样本，假定都服从高斯分布，根据训练样本，估计出先验概率以及高斯分布的均值和协方差矩阵（注意这里两类内部高斯分布的协方差矩阵相同），即可通过贝叶斯公式求出一个新样本分别属于两类的概率，进而可实现对该样本的分类。GDA详细推导如P6，那么高斯判别分析的核心工作就是估计未知量ϕ,μ0,μ1,Σ。如何来估计这些参数？又该最大似然估计上场了。其对数似然函数如P6所示，推导如P7。 线性分类 线性分类 P8最大似然函数延申 &amp; P9朴素贝叶斯 上面的推导似乎很复杂，但其结果却是非常简洁。通过上述公式，所有的参数都已经估计出来，需要判断一个新样本x时，可分别使用贝叶斯求出p(y=0|x)和p(y=1|x)，取概率更大的那个类。实际计算时，我们只需要比大小，那么贝叶斯公式中分母项可以不计算，由于2个高斯函数协方差矩阵相同，则高斯分布前面那相同部分也可以忽略。实际上，GDA算法也是一个线性分类器。 贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。 决策的目标是最小化分类错误率，贝叶斯最优分类器要对每个样本x，选择能使后验概率P(c|x)最大的类别c标记。在现实任务中后验概率通常难以直接获得。从这个角度来说，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率 P(c|x)。大体来说，主要有两种策略：给定x，可通过直接建模P(c|x)来预测c，这样得到的是“判别式模型”，例如，决策树、BP神经网络、支持向量机等等；也可先对联合概率分布P(x,c)建模，然后在由此获得P(c|x)，这样得到的是“生成式模型”。 贝叶斯估计中类先验概率P(c)表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，P(c)可通过各类样本出现的频率来进行估计。因为对于类条件概率P(x|c)来说，由于它涉及关于x所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。假如样本的d个属性都是二值的，则样本空间将有2的d次方种可能取值，在现实中，这个种类数往往大于训练样本，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计P(x|c)显然不可行，因为“未被观测到”与“出现概率为零”通常是不同的。这可以通过极大似然估计来解决。 基于贝叶斯公式来估计后验概率P(c|x)的主要困难在于：类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。因此朴素贝叶斯分类器采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。也就是说，假设每个属性独立的对分类结果发生影响。 线性分类","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yinqm.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学理论","slug":"数学理论","permalink":"http://yinqm.com/tags/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"}]},{"title":"MachineLearning01","slug":"MachineLearning01","date":"2020-03-21T12:24:33.000Z","updated":"2020-03-21T16:40:12.550Z","comments":true,"path":"2020/03/21/MachineLearning01/","link":"","permalink":"http://yinqm.com/2020/03/21/MachineLearning01/","excerpt":"","text":"Frequentist VS Bayesian&emsp;&emsp;在机器学习领域分为两个流派，分别是贝叶斯派和频率派。两种学派所基于的理论背景不同，应用场景也不尽相同。在频率派和贝叶斯派两种理论派别中，似然函数p(D|w)是问题的中心所在，但是对于其被运用的方式方面有着本质的区别。 Frequentist&emsp;&emsp;在频率派中，W被认为是一个固定的数值，其数值的计算是通过对于训练集的学习和估计。这个思想即神经网络中常用的：建立模型，选择损失函数，优化目标函数，将损失降到可以度量范围内的最小值，从而得到对w参数的估计。而在贝叶斯派看来，只有通过观察w的概率分布，才能表示参数的不确定度，即对于w的数值是不确定的。在频率派中一个被广泛使用的用来估计的工具是：最大化似然函数。 Bayesian&emsp;&emsp;贝叶斯派中的一个优势在于对于先验知识的包含是逐步上升的。比如，对一枚质地均匀的硬币进行三次投掷，结果都是反面，则在贝叶斯派的最大化似然函数看来，反面的概率是1. 而对于贝叶斯派来说，由于具有先验知识（质地均匀，先验概率为1/2），其运算结果将不会像频率派一样极端。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yinqm.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"AI科普","slug":"AI科普","permalink":"http://yinqm.com/tags/AI%E7%A7%91%E6%99%AE/"}]},{"title":"MachineLearning02","slug":"MachineLearning02","date":"2020-03-20T09:42:52.000Z","updated":"2020-03-21T16:39:37.073Z","comments":true,"path":"2020/03/20/MachineLearning02/","link":"","permalink":"http://yinqm.com/2020/03/20/MachineLearning02/","excerpt":"","text":"机器学习中常用的数学基础 P1高斯分布（正态分布） 自然界产生的数据分布一般是正态分布（如年龄、身高、体重等），故当对数据不清楚其潜在的结构、即对数据潜在分布模式不明确时，近似采用正态分布。 在机器学习中，目标通常是使得数据线性可分，甚至意味着将数据投影到更高维空间，找到一个可拟合的超平面（如SVM核，神经网络层，softmax等）。原因是“线性分界通常有助于减少方差variance而且是最简单，自然和可理解的”，同时减少数学、计算的复杂性。同时，当我们聚焦线性可分时，通常可以很好减少异常点、影响点和杠杆点的作用。因为超平面是对影响点和杠点（异常点）非常敏感。举个例子，在二维空间中，我们有一个预测器predictor(X)，和目标值（y)，假设X和y是很好的正相关。在这个情形下，假设X是正态分布，y也是正态分布，那么你可以拟合到一条很直的线，相比边界点（异常点，杠杆点），很多点都集中在线的中间，所以这个预测回归线在预测未知数据时，降低方差的影响。 数学基础 P2有偏估计&amp;无偏估计 本质来讲，无偏/无偏估计是指估算统计量的公式，无偏估计就是可以预见，多次采样计算的统计量是在真实值左右两边。类似于正态分布的钟型图形。比如对于均值μ估计。一定有的比μ大，有的比μ小。那么对于有偏估计，就是多次采样，估算的统计量将会在真实值的一侧（都是大于或者都是小于真实值）无偏估计并不一定比有偏估计更加”有效”，因为所谓估算有效是指更加靠近真实值。 数学基础 P3二次型 二次型：n个变量的二次多项式称为二次型，即在一个多项式中，未知数的个数为任意多个，但每一项的次数都为2的多项式。线性代数的重要内容之一，它起源于几何学中二次曲线方程和二次曲面方程化为标准形问题的研究。二次型理论与域的特征有关。PCA跟特征值和特征向量有关，应该跟二次型有关。 数学基础 P4二次型 &amp;&amp; P5： PCA主成分分析用到实对称阵的相似对角化。PCA的目的是降噪和去冗余，是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。 数学基础 P6协方差（Covariance） 在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。用于机器学习中衡量多个特征之间的关系。 数学基础 P7Jensen不等式（Jensen’s inequality） 在概率论、机器学习、测度论、统计物理等领域都有相关应用。在机器学习领域，用Jensen不等式用来证明KL散度大于等于0,EM算法推导最后也用到了Jensen不等式。 数学基础","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yinqm.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学理论","slug":"数学理论","permalink":"http://yinqm.com/tags/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"}]},{"title":"2020第一篇随笔","slug":"2020第一篇随笔","date":"2020-03-15T18:20:09.000Z","updated":"2020-03-21T16:40:55.705Z","comments":true,"path":"2020/03/16/2020第一篇随笔/","link":"","permalink":"http://yinqm.com/2020/03/16/2020%E7%AC%AC%E4%B8%80%E7%AF%87%E9%9A%8F%E7%AC%94/","excerpt":"","text":"&lt;font size=5,face=”微软雅黑”&gt;&emsp;&emsp;兴致所至，做一个自己的网站，起名破站，敲代码水平有限，做得不咋样，但总算可以正常营业了，斯是陋室，惟吾德馨嘛。&emsp;&emsp;“好记性不如烂笔头”，想把自己在学习中遇到的困惑，bug，以及自己最后的解决办法分享出来，互相学习借鉴，如果大家愿意来看，且能够解其惑，自然是相当棒的！其次也可以加深自身印象，在以后的日子里遇到同样的问题，能够更加迅速的解决。&emsp;&emsp;这个博客网站呢，既是为了方便学习用的，也是记录生活。黎明将至，长话短说，总之纪念一下我的小破站做好了，Hello New World！&emsp;&emsp;虽然这个博客网站是因学习而开，但是却也因此打开了新世界的大门，并不愿关上。&emsp;&emsp;愿，脚踏实地，未来可期，&emsp;&emsp;望，法不阿贵，山河无恙。&emsp;&emsp;（Markdown+LaTeX组合写博文确实不错）","categories":[],"tags":[{"name":"杂记","slug":"杂记","permalink":"http://yinqm.com/tags/%E6%9D%82%E8%AE%B0/"},{"name":"心情","slug":"心情","permalink":"http://yinqm.com/tags/%E5%BF%83%E6%83%85/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-03-14T11:23:15.786Z","updated":"2020-03-14T11:23:15.786Z","comments":true,"path":"2020/03/14/hello-world/","link":"","permalink":"http://yinqm.com/2020/03/14/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}